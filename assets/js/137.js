(window.webpackJsonp=window.webpackJsonp||[]).push([[137],{680:function(t,e,a){"use strict";a.r(e);var s=a(5),n=Object(s.a)({},(function(){var t=this,e=t.$createElement,a=t._self._c||e;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("p",[t._v("I've always been fascinated by religious holy texts - they're such a product of the time they are written, and generally have little or no useful information for how to live one's life, unless you're willing to \"interpret\" them with a distorting lens that helps to bring them in line with modern ideas of how we should behave and treat each other. But, if I were to create a new religion, or cult, how would I go about it? Writing an entire book seems like hard work, so what if I could train some kind of deep learning bot on existing religious texts and then ask it to use its knowledge to create a new holy book.")]),t._v(" "),a("p",[t._v("I decided to use the now outdated GPT-2 algorithm for this task, as it's free and the model is small enough that it can run on my laptop without taking too long. Although I have a decent graphics card in my gaming PC (an NVidia 3060 Ti), just installing the Python requirements for the CPU version of GPT-2 was arduous enough - I gave up on the idea of ever getting the CUDA version working.")]),t._v(" "),a("p",[t._v("I found the GPT-2-Simple project, wrestled with its very specific python library requirements, and eventually managed to get it running.")]),t._v(" "),a("p",[t._v("Here are the source books I downloaded:")]),t._v(" "),a("ul",[a("li",[a("a",{attrs:{href:"https://www.gutenberg.org/files/17/17-0.txt",target:"_blank",rel:"noopener noreferrer"}},[t._v("The Book of Mormon"),a("OutboundLink")],1)]),t._v(" "),a("li",[a("a",{attrs:{href:"https://www.gutenberg.org/files/30/30.txt",target:"_blank",rel:"noopener noreferrer"}},[t._v("The Bible"),a("OutboundLink")],1)]),t._v(" "),a("li",[a("a",{attrs:{href:"https://www.gutenberg.org/files/124/124.txt",target:"_blank",rel:"noopener noreferrer"}},[t._v("The Apocrypha"),a("OutboundLink")],1)]),t._v(" "),a("li",[a("a",{attrs:{href:"https://www.gutenberg.org/files/216/216.txt",target:"_blank",rel:"noopener noreferrer"}},[t._v("The Tao Teh King"),a("OutboundLink")],1)]),t._v(" "),a("li",[a("a",{attrs:{href:"https://www.gutenberg.org/files/2017/2017.txt",target:"_blank",rel:"noopener noreferrer"}},[t._v("The Dhammapada"),a("OutboundLink")],1)]),t._v(" "),a("li",[a("a",{attrs:{href:"https://www.gutenberg.org/files/2388/2388.txt",target:"_blank",rel:"noopener noreferrer"}},[t._v("The Bhagavad-Gita"),a("OutboundLink")],1)]),t._v(" "),a("li",[a("a",{attrs:{href:"https://www.gutenberg.org/files/7440/7440.txt",target:"_blank",rel:"noopener noreferrer"}},[t._v("The Quran"),a("OutboundLink")],1)]),t._v(" "),a("li",[a("a",{attrs:{href:"https://www.gutenberg.org/files/3283/3283-8.txt",target:"_blank",rel:"noopener noreferrer"}},[t._v("The Upanishads"),a("OutboundLink")],1)]),t._v(" "),a("li",[a("a",{attrs:{href:"https://www.gutenberg.org/files/3458/3458.txt",target:"_blank",rel:"noopener noreferrer"}},[t._v("Science and Health With Key to the Scriptures"),a("OutboundLink")],1)]),t._v(" "),a("li",[a("a",{attrs:{href:"https://www.gutenberg.org/files/16523/16523-0.txt",target:"_blank",rel:"noopener noreferrer"}},[t._v("The KitÃ¡b-i-Aqdas"),a("OutboundLink")],1)]),t._v(" "),a("li",[a("a",{attrs:{href:"https://www.gutenberg.org/files/24737/24737.txt",target:"_blank",rel:"noopener noreferrer"}},[t._v("The Children of Odin"),a("OutboundLink")],1)])]),t._v(" "),a("p",[t._v("First I stitched the books I'd grabbed from the Gutenberg project together with some Node JS code:")]),t._v(" "),a("div",{staticClass:"language-javascript extra-class"},[a("pre",{pre:!0,attrs:{class:"language-javascript"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("readdirSync"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" readFileSync"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" writeFileSync"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("require")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'fs'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" text "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("readdirSync")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'./data'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("map")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token parameter"}},[t._v("file")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=>")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'<|startoftext|>'")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("readFileSync")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'./data/'")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" file"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'<|endoftext|>'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("writeFileSync")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'holy.txt'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" text"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("join")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'\\n'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),a("p",[t._v("I then used one piece of Python code to train the model with my religious texts:")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" os\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" gpt_2_simple "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" gpt2\n\nmodel_name "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"355M"')]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("not")]),t._v(" os"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("isdir"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("os"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("join"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"models"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" model_name"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n\t"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string-interpolation"}},[a("span",{pre:!0,attrs:{class:"token string"}},[t._v('f"Downloading ')]),a("span",{pre:!0,attrs:{class:"token interpolation"}},[a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("model_name"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")])]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v(' model..."')])]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\tgpt2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("download_gpt2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("model_name "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" model_name"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nsess "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" gpt2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("start_tf_sess"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ngpt2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("finetune"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sess"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"texts.txt"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" model_name "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" model_name"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" run_name "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'holy'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" steps "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1000")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("p",[t._v("And a second piece of Python code to generate some text:")]),t._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" gpt_2_simple "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" gpt2\n\nsess "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" gpt2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("start_tf_sess"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ngpt2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("load_gpt2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sess"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" run_name "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'holy'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\ngpt2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("generate"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sess"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" run_name "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'holy'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),a("p",[t._v("I was pretty happy with my results:")]),t._v(" "),a("blockquote",[a("p",[t._v("3:005:008 Thou art the King's counsellor, and I the King his counsellor, and thy word is truth, and I the King's counsellor, and thy word is justice; but the words of the wicked and the wicked are in your mouth: and thou shalt not deliver them, and I the King will deliver thee: so tell me the words of the wicked, as the wickedness of my soul: and thou shalt say, If, when thou art king, thou keepest the commandments of my conscience: then thou art my brother, and I shall not be ashamed.")])]),t._v(" "),a("blockquote",[a("p",[t._v("20:2 And the people of Judah went down to the place, and went forth to fight against the people of Jerusalem. Nevertheless, before they did so, they did it with great courage; but not of the same courage was they like unto the city of the children of Israel.\n20:3 And the people of Jerusalem fought not as the people of Judah did: but they were not of the same spirit, because the LORD of hosts dealt a dispute with the people of Judah.\n20:4 And the people of this city did not rise up against the people, neither went to Jerusalem with the battle; but they went down, and returned as they went down, and departed, and went their way.\n20:5 Thus they were scattered, with their tents; and it was not according to their numbers for that they were so scattered in like manner; but according to their number they went up against them, and did battle.")])]),t._v(" "),a("blockquote",[a("p",[t._v("They had seen his face from their youth, and were afraid. He had shown great power on the field; their fear was not so great. They found a place that they could put distance between the brother and his master in a distance that was ten paces. He was able to make his brother a prisoner, to cut off their right hand, and to put their master to the death by hanging. His brother then went with his master to the field, and told the people that there were two men at the door, and that the brothers had been there four or five minutes.")])]),t._v(" "),a("blockquote",[a("p",[t._v("It is my advice therefore to make use of all means whereby I am able to make use of my intelligence in this matter. For I feel sure that the problems that lie before thee, though greater than any before, and worse still, than were before, and yet, though less, are all to be expected from the evil and wickedness that are in his sight.")])]),t._v(" "),a("blockquote",[a("p",[t._v("The Supreme and Immaterial Principle\nTo-day science reveals the Universe to be subject to the law of relativity, thus, in the sense of a flat, constant state, and yet that the Universe also changes for various conditions, - and this also indicates that matter, is not to blame for the existence of God. We do not find a continuity of matter in matter or a succession in matter over Matter. All that exists does so in degrees and that the higher degree of being leads to other degrees and that all that takes place rests upon this higher level of being.")])]),t._v(" "),a("p",[t._v("It appears that, rather than synthesising all the different styles in each of the books I fed the software, it has learned each of the styles separately and is capable of generating text to emulate each of them. I'll have to spend some time figuring out how to get it to give me a concise block of combined \"wisdom\" from all of the books it's consumed.")]),t._v(" "),a("p",[t._v("I've also trained this code on a back catalog of QAnon posts, and it's eerie just how well it emulates that particular crazy right wing conspiracy. But I'll leave the results of that experiment for another blog post.")])])}),[],!1,null,null,null);e.default=n.exports}}]);